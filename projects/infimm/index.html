<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> InfiMM | </title> <meta name="author" content="Quanzeng You"> <meta name="description" content="Advancing Multimodal Understanding from Flamingo's Legacy through Diverse LLM Integration"> <meta name="keywords" content="Computer Vison, AI, Machine Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qzyou.github.io/projects/infimm/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">InfiMM</h1> <p class="post-description">Advancing Multimodal Understanding from Flamingo's Legacy through Diverse LLM Integration</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/infimm/assets_infimm-logo.webp-480.webp 480w,/assets/img/infimm/assets_infimm-logo.webp-800.webp 800w,/assets/img/infimm/assets_infimm-logo.webp-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/infimm/assets_infimm-logo.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Examples from our benchmark" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> </div> </div> <h2 id="overview">Overview</h2> <p>InfiMM, inspired by the Flamingo architecture, sets itself apart with unique training data and diverse large language models (LLMs). This approach allows InfiMM to maintain the core strengths of Flamingo while offering enhanced capabilities. As the premier open-sourced variant in this domain, InfiMM excels in accessibility and adaptability, driven by community collaboration. It’s more than an emulation of Flamingo; it’s an innovation in visual language processing.</p> <p>Our model is another attempt to produce the result reported in the paper “Flamingo: A Large-scale Visual Language Model for Multimodal Understanding” by DeepMind. Compared with previous open-sourced attempts (<a href="https://github.com/mlfoundations/open_flamingo" rel="external nofollow noopener" target="_blank">OpenFlamingo</a> and <a href="https://huggingface.co/blog/idefics" rel="external nofollow noopener" target="_blank">IDEFIC</a>, InfiMM offers a more flexible models, allowing for a wide range of applications. In particular, InfiMM integrates the latest LLM models into VLM domain the reveals the impact of LLMs with different scales and architectures.</p> <p>Please note that InfiMM is currently in beta stage and we are continuously working on improving it.</p> <h2 id="model-details">Model Details</h2> <ul> <li> <strong>Developed by</strong>: Institute of Automation, Chinese Academy of Sciences and ByteDance</li> <li> <strong>Model Type</strong>: Visual Language Model (VLM)</li> <li> <strong>Language</strong>: English</li> <li> <strong>LLMs</strong>: <a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta" rel="external nofollow noopener" target="_blank">Zephyr</a>, <a href="https://ai.meta.com/llama/" rel="external nofollow noopener" target="_blank">LLaMA2-13B</a>, <a href="https://huggingface.co/lmsys/vicuna-13b-v1.5" rel="external nofollow noopener" target="_blank">Vicuna-13B</a> </li> <li> <strong>Vision Model</strong>: <a href="https://huggingface.co/QuanSun/EVA-CLIP" rel="external nofollow noopener" target="_blank">EVA CLIP</a> </li> <li> <strong>Language(s) (NLP):</strong> en</li> <li> <strong>License:</strong> see <a href="#license">License section</a> </li> </ul> <h2 id="evaluation">Evaluation</h2> <h3 id="pretraining-evaluation">PreTraining Evaluation</h3> <p>We evaluate the pretrained models on the following downstream tasks: Image Captioning and VQA. We also compare with our results with <a href="https://huggingface.co/blog/idefics" rel="external nofollow noopener" target="_blank">IDEFICS</a>.</p> <table> <thead> <tr> <th>Model</th> <th>Shots</th> <th>COCO CIDEr</th> <th>Flickr30K CIDEr</th> <th>VQA v2 Acc</th> <th>TextVQA Acc</th> <th>OK-VQA Acc</th> </tr> </thead> <tbody> <tr> <td>IDEFICS-9B</td> <td>0</td> <td>46</td> <td>27.3</td> <td>50.9</td> <td>25.9</td> <td>38.4</td> </tr> <tr> <td> </td> <td>4</td> <td>93</td> <td>59.7</td> <td>55.4</td> <td>27.6</td> <td>45.5</td> </tr> <tr> <td>IDEFICS-80B</td> <td>0</td> <td>91.8</td> <td>53.7</td> <td>60</td> <td>30.9</td> <td>45.2</td> </tr> <tr> <td> </td> <td>4</td> <td>110.3</td> <td>73.7</td> <td>64.6</td> <td>34.4</td> <td>52.4</td> </tr> <tr> <td>InfiMM-Zephyr-7B</td> <td>0</td> <td>78.8</td> <td>60.7</td> <td>33.7</td> <td>15.2</td> <td>17.1</td> </tr> <tr> <td> </td> <td>4</td> <td>108.6</td> <td>71.9</td> <td>59.1</td> <td>34.3</td> <td>50.5</td> </tr> <tr> <td>InfiMM-Llama2-13B</td> <td>0</td> <td>85.4</td> <td>54.6</td> <td>51.6</td> <td>24.2</td> <td>26.4</td> </tr> <tr> <td> </td> <td>4</td> <td>125.2</td> <td>87.1</td> <td>66.1</td> <td>38.2</td> <td>55.5</td> </tr> <tr> <td>InfiMM-Vicuna13B</td> <td>0</td> <td>69.6</td> <td>49.6</td> <td>60.4</td> <td>32.8</td> <td>49.2</td> </tr> <tr> <td> </td> <td>4</td> <td>118.1</td> <td>81.4</td> <td>64.2</td> <td>38.4</td> <td>53.7</td> </tr> </tbody> </table> <h3 id="ift-evaluation">IFT Evaluation</h3> <p>In our analysis, we concentrate on two primary benchmarks for evaluating MLLMs: 1) Multi-choice Question Answering (QA) and 2) Open-ended Evaluation. We’ve observed that the evaluation metrics for tasks like Visual Question Answering (VQA) and Text-VQA are overly sensitive to exact answer matches. This approach can be misleading, particularly when models provide synonymous but technically accurate responses. Therefore, these metrics have been omitted from our comparison for a more precise assessment. The evaluation results are shown in the table below.</p> <table> <thead> <tr> <th>Model</th> <th>ScienceQA-Img</th> <th>MME</th> <th>MM-VET</th> <th>InfiMM-Eval</th> <th>MMbench</th> <th>MMMU-Val</th> <th>MMMU-Test</th> </tr> </thead> <tbody> <tr> <td>Otter-9B</td> <td>-</td> <td>1292/306</td> <td>24.6</td> <td>32.2</td> <td>-</td> <td>22.69</td> <td>-</td> </tr> <tr> <td>IDEFICS-9B-Instruct</td> <td>60.6</td> <td>-/-</td> <td>-</td> <td>-</td> <td>-</td> <td>24.53</td> <td>-</td> </tr> <tr> <td>InfiMM-Zephyr-7B</td> <td>71.1</td> <td>P: 1406<br>C:327</td> <td>32.8</td> <td>36.0</td> <td>59.7</td> <td>39.4</td> <td>35.5</td> </tr> <tr> <td>InfiMM-Llama-13b</td> <td>73.0</td> <td>P: 1444.5<br>C: 337.6</td> <td>39.2</td> <td>0.4559/0.414</td> <td>66.4</td> <td>39.1</td> <td>35.2</td> </tr> <tr> <td>InfiMM-Vicuna-13B</td> <td>74.0</td> <td>P: 1461.2<br>C: 323.5</td> <td>36.0</td> <td>40.0</td> <td>66.7</td> <td>37.6</td> <td>34.6</td> </tr> </tbody> </table> <h2 id="links">Links</h2> <p><a href="https://huggingface.co/Infi-MM" rel="external nofollow noopener" target="_blank">Project HomePage</a></p> <p><a href="https://huggingface.co/Infi-MM/infimm-zephyr" rel="external nofollow noopener" target="_blank">infimm-zephyr</a></p> <p><a href="https://huggingface.co/Infi-MM/infimm-vicuna13b" rel="external nofollow noopener" target="_blank">infimm-vicuna13b</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Quanzeng You. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 09, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>